{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6hL_y9eHzPe"
      },
      "outputs": [],
      "source": [
        "# Ensure necessary libraries are installed\n",
        "!pip install transformers datasets pandas\n",
        "\n",
        "# Import required libraries\n",
        "from google.colab import files\n",
        "from IPython.display import display # Import display\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "import os\n",
        "import torch # Import torch\n",
        "\n",
        "# --- Step 1: Upload the dataset (if needed) ---\n",
        "# If you have already uploaded cleaned_samsum.csv to your Colab environment\n",
        "# or mounted Google Drive where it resides, you can skip this upload step.\n",
        "# Otherwise, uncomment the following lines to upload the file.\n",
        "# print(\"Please upload the 'cleaned_samsum.csv' file.\")\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# --- Step 2: Load and prepare the data ---\n",
        "\n",
        "# Define the path to your CSV file\n",
        "csv_file_path = \"cleaned_samsum.csv\"\n",
        "\n",
        "# Load the CSV into a Pandas DataFrame\n",
        "df = None # Initialize df to None before the try block\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "    print(f\"Successfully loaded {csv_file_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
        "    print(\"Please upload the file using the 'files.upload()' function or ensure it's in the correct path.\")\n",
        "    # Exiting the cell here because subsequent steps depend on the dataframe\n",
        "    # If you want to handle this differently (e.g., just print error and continue\n",
        "    # but skip dependent steps), you would need more complex logic.\n",
        "    # For this example, we'll keep the exit to match the original intent\n",
        "    # of stopping if the file is not found.\n",
        "    exit()\n",
        "\n",
        "# Only proceed if df was successfully loaded\n",
        "if df is not None:\n",
        "    # Keep only the cleaned columns and rename them\n",
        "    df = df[['cleaned_text', 'cleaned_summary']].rename(columns={\n",
        "        'cleaned_text': 'input',\n",
        "        'cleaned_summary': 'target'\n",
        "    })\n",
        "    print(\"DataFrame prepared with 'input' and 'target' columns.\")\n",
        "    print(\"First 5 rows of the prepared DataFrame:\")\n",
        "    display(df.head()) # Use display for better formatting in Colab\n",
        "\n",
        "    # Convert the Pandas DataFrame to a Hugging Face Dataset\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    print(f\"\\nHugging Face Dataset created. Total samples: {len(dataset)}\")\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    dataset = dataset.train_test_split(test_size=0.1, seed=42) # Added seed for reproducibility\n",
        "    print(f\"Dataset split into training ({len(dataset['train'])} samples) and test ({len(dataset['test'])} samples).\")\n",
        "\n",
        "    # --- Step 3: Load the Tokenizer ---\n",
        "\n",
        "    # Define the model checkpoint for the tokenizer and model\n",
        "    model_checkpoint = \"t5-small\"\n",
        "    print(f\"\\nLoading tokenizer from: {model_checkpoint}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- Step 4: Tokenize the Dataset ---\n",
        "\n",
        "    # Define the preprocessing function\n",
        "    def preprocess(example):\n",
        "        \"\"\"Tokenizes input text and target summaries.\"\"\"\n",
        "        # Define max lengths for input and target - adjusted based on typical summary tasks\n",
        "        max_input_length = 512 # Original was 512, keeping it\n",
        "        max_target_length = 128 # Original was 128, keeping it\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            example[\"input\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_input_length\n",
        "        )\n",
        "        targets = tokenizer(\n",
        "            example[\"target\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_target_length\n",
        "        )\n",
        "        # Set the tokenized target as labels for training\n",
        "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "        return inputs\n",
        "\n",
        "    print(\"\\nApplying preprocessing (tokenization) to the dataset...\")\n",
        "    # Apply the preprocess function to the dataset\n",
        "    tokenized_dataset = dataset.map(\n",
        "        preprocess,\n",
        "        batched=True,          # Process in batches for efficiency\n",
        "        remove_columns=[\"input\", \"target\"] # Remove original text columns\n",
        "    )\n",
        "    print(\"Dataset tokenization complete.\")\n",
        "    print(\"Example of a tokenized sample (showing input_ids and labels):\")\n",
        "    # Print a sample from the tokenized dataset - adjust index as needed\n",
        "    print(tokenized_dataset[\"train\"][0].keys())\n",
        "\n",
        "\n",
        "    # --- Step 5: Load the Model ---\n",
        "\n",
        "    print(f\"\\nLoading model from: {model_checkpoint}\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # --- Step 6: Set Training Arguments ---\n",
        "\n",
        "    print(\"\\nSetting up training arguments...\")\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=\"./results\",             # Output directory for model checkpoints and results\n",
        "        eval_strategy=\"epoch\",              # Evaluate at the end of each epoch\n",
        "        learning_rate=2e-4,                 # Learning rate\n",
        "        per_device_train_batch_size=8,      # Batch size for training on each device\n",
        "        per_device_eval_batch_size=8,       # Batch size for evaluation on each device\n",
        "        weight_decay=0.01,                  # Weight decay for regularization\n",
        "        save_total_limit=3,                 # Limit the total number of saved checkpoints\n",
        "        num_train_epochs=3,                 # Total number of training epochs\n",
        "        predict_with_generate=True,         # Use generation for prediction (required for summarization)\n",
        "        logging_dir=\"./logs\",               # Directory for logging\n",
        "        logging_steps=10,                   # Log training information every X steps\n",
        "        fp16=True,                          # Enable mixed precision training (requires GPU)\n",
        "        report_to=[\"none\"]                  # Explicitly disable reporting to any online service\n",
        "    )\n",
        "    print(\"Training arguments set.\")\n",
        "    # Check for GPU availability and warn if fp16 is enabled without one\n",
        "    if training_args.fp16 and not torch.cuda.is_available():\n",
        "        print(\"\\nWarning: fp16 is enabled but no GPU found. Training will likely fail or be very slow. Set fp16=False if not using GPU.\")\n",
        "\n",
        "\n",
        "    # --- Step 7: Define the Trainer ---\n",
        "\n",
        "    # Get train/val splits from the tokenized dataset\n",
        "    train_dataset = tokenized_dataset[\"train\"]\n",
        "    val_dataset = tokenized_dataset[\"test\"]\n",
        "\n",
        "    # Define data collator for Seq2Seq tasks\n",
        "    # This collator pads the batches dynamically to the longest sequence in the batch\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "    print(\"\\nData collator defined.\")\n",
        "\n",
        "    # Disable Weights & Biases logging if not needed\n",
        "    # This environment variable also helps in some cases\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "    print(\"WANDB logging disabled.\")\n",
        "\n",
        "    # Define the Seq2Seq Trainer\n",
        "    print(\"Defining the Seq2SeqTrainer...\")\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,                  # The loaded model\n",
        "        args=training_args,           # The training arguments\n",
        "        train_dataset=train_dataset,  # The training dataset\n",
        "        eval_dataset=val_dataset,     # The evaluation dataset\n",
        "        tokenizer=tokenizer,          # The tokenizer\n",
        "        data_collator=data_collator,  # The data collator\n",
        "    )\n",
        "    print(\"Trainer defined.\")\n",
        "\n",
        "    # --- Step 8: Train the Model ---\n",
        "\n",
        "    print(\"\\nStarting model training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training finished successfully!\")\n",
        "\n",
        "    # --- Step 9: Save the Model and Tokenizer ---\n",
        "\n",
        "    # Define the directory to save the fine-tuned model\n",
        "    save_directory = \"./t5-finetuned-model\"\n",
        "\n",
        "    print(f\"\\nSaving the fine-tuned model and tokenizer to '{save_directory}'...\")\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "    # Save the model\n",
        "    trainer.save_model(save_directory)\n",
        "\n",
        "    # Save the tokenizer\n",
        "    tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "    print(\"Model and tokenizer saved.\")\n",
        "\n",
        "    # You can optionally verify the saved files\n",
        "    # !ls ./t5-finetuned-model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./t5-finetuned-model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaqSDmlXG-YY",
        "outputId": "c465dbb1-e07a-44e1-97bc-7ad157b19aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\tspecial_tokens_map.json  tokenizer.json\n",
            "generation_config.json\tspiece.model\t\t training_args.bin\n",
            "model.safetensors\ttokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary libraries are installed (already done)\n",
        "# !pip install zip\n",
        "\n",
        "# Zip the saved model directory\n",
        "print(\"Zipping the fine-tuned model directory...\")\n",
        "# -r: recursive (include subdirectories)\n",
        "# ./t5-finetuned-model.zip: name of the output zip file\n",
        "# ./t5-finetuned-model: the directory to zip\n",
        "!zip -r t5-finetuned-model.zip ./t5-finetuned-model\n",
        "\n",
        "print(\"\\nModel directory zipped successfully!\")\n",
        "print(\"You can now download 't5-finetuned-model.zip' from the Colab file browser sidebar.\")"
      ],
      "metadata": {
        "id": "gxGPlnLAIDkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lPK2KOkcIE_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}